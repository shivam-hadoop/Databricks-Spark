{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Creating Spark Session and Spark Context Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.types._\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@13c601e5\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "val spark:SparkSession = SparkSession.builder().master(\"local\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dummy DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+\n",
      "|   city|  country|population|\n",
      "+-------+---------+----------+\n",
      "| Boston|      USA|      0.67|\n",
      "|  Dubai|      UAE|       3.1|\n",
      "|Cordoba|Argentina|      1.39|\n",
      "+-------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "df: org.apache.spark.sql.DataFrame = [city: string, country: string ... 1 more field]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val df = Seq((\"Boston\", \"USA\", 0.67),(\"Dubai\", \"UAE\", 3.1),(\"Cordoba\", \"Argentina\", 1.39)).\n",
    "toDF(\"city\",\"country\",\"population\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a new column to an existing DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------------------------+\n",
      "|   city|  country|population|is_population_in_billion|\n",
      "+-------+---------+----------+------------------------+\n",
      "| Boston|      USA|      0.67|                   false|\n",
      "|  Dubai|      UAE|       3.1|                    true|\n",
      "|Cordoba|Argentina|      1.39|                    true|\n",
      "+-------+---------+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"is_population_in_billion\",$\"population\" > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|  city|country|population|\n",
      "+------+-------+----------+\n",
      "|Boston|    USA|      0.67|\n",
      "+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter($\"population\" < 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a local CSV File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val catsFilePath = \"/Users/shivam/Downloads/server/files/cats.csv\"\n",
    "val catsDF = spark.read.option(\"header\",true).csv(catsFilePath)\n",
    "catsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------+\n",
      "|superhero|city    |cityStratingWithNew|\n",
      "+---------+--------+-------------------+\n",
      "|thor     |new york|true               |\n",
      "|aquaman  |atlantis|false              |\n",
      "|wolverine|new york|true               |\n",
      "+---------+--------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [superhero: string, city: string]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq((\"thor\", \"new york\"),(\"aquaman\", \"atlantis\"),(\"wolverine\", \"new york\")).toDF(\"superhero\", \"city\")\n",
    "df.withColumn(\"cityStratingWithNew\",$\"city\".startsWith(\"new\")).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+\n",
      "|num|word|new_num|\n",
      "+---+----+-------+\n",
      "| 10| cat|     15|\n",
      "|  4| dog|      9|\n",
      "|  7|null|     12|\n",
      "+---+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [num: int, word: string]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq((10, \"cat\"),(4, \"dog\"),(7, null)).toDF(\"num\", \"word\")\n",
    "df.withColumn(\"new_num\",$\"num\" + lit(5)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null and is Not Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newDF: org.apache.spark.sql.DataFrame = [num: int, word: string ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newDF = df.withColumn(\"isWordNull\",$\"word\".isNull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "|num|word|isWordNull|\n",
      "+---+----+----------+\n",
      "| 10| cat|     false|\n",
      "|  4| dog|     false|\n",
      "|  7|  NA|      true|\n",
      "+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDF.na.fill(\"NA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When and Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------------+\n",
      "|word1|word2|         status|\n",
      "+-----+-----+---------------+\n",
      "|  bat|  bat|     same_words|\n",
      "|snake|  rat|word1 is bigger|\n",
      "|  cup|phone|  Let's Default|\n",
      "|  key| null|  Let's Default|\n",
      "+-----+-----+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [word1: string, word2: string]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq((\"bat\", \"bat\"),(\"snake\", \"rat\"),(\"cup\", \"phone\"),(\"key\", null)).toDF(\"word1\", \"word2\")\n",
    "\n",
    "df.withColumn(\"status\",when($\"word1\" === $\"word2\",\"same_words\").when\n",
    "             (length($\"word1\") > length($\"word2\"),\"word1 is bigger\").otherwise(\"Let's Default\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL Build in Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|numbers|factorial|\n",
      "+-------+---------+\n",
      "|      1|        1|\n",
      "|      2|        2|\n",
      "|      3|        6|\n",
      "|      4|       24|\n",
      "|      5|      120|\n",
      "+-------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "numbers: org.apache.spark.sql.DataFrame = [numbers: int]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val numbers = Seq((1),(2),(3),(4),(5)).toDF(\"numbers\")\n",
    "numbers.withColumn(\"factorial\",factorial($\"numbers\")).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When and otherwise Revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|age|      status|\n",
      "+---+------------+\n",
      "| 10|       child|\n",
      "| 15|semiTeenager|\n",
      "| 25|       adult|\n",
      "+---+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [age: int]\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(10, 15, 25).toDF(\"age\")\n",
    "df.withColumn(\"status\",when(($\"age\" < 15),\"child\").when(($\"age\" < 18 && $\"age\" > 14),\"semiTeenager\")\n",
    "             .otherwise(\"adult\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a UDF for above age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Column\n",
       "ageStatus: (age: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "def ageStatus(age:Column):Column ={\n",
    "    when(($\"age\" < 15),\"child\").when(($\"age\" < 18 && $\"age\" > 14),\"semiTeenager\").otherwise(\"adult\") \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|age|      status|\n",
      "+---+------------+\n",
      "| 10|       child|\n",
      "| 15|semiTeenager|\n",
      "| 25|       adult|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"status\",ageStatus($\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF that removes all whitespaces and lowercase all characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Failed to execute user defined function($read$$Lambda$2059/45495881: (string) => string)",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Failed to execute user defined function($read$$Lambda$2059/45495881: (string) => string)",
      "  at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1129)",
      "  at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:156)",
      "  at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:83)",
      "  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$17.$anonfun$applyOrElse$71(Optimizer.scala:1508)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)",
      "  at scala.collection.immutable.List.map(List.scala:298)",
      "  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$17.applyOrElse(Optimizer.scala:1508)",
      "  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$17.applyOrElse(Optimizer.scala:1503)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)",
      "  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1503)",
      "  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1502)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:149)",
      "  at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)",
      "  at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)",
      "  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:146)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:138)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:138)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:116)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:116)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:82)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:133)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:133)",
      "  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:82)",
      "  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:79)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$writePlans$4(QueryExecution.scala:197)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$.append(QueryPlan.scala:381)",
      "  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$writePlans(QueryExecution.scala:197)",
      "  at org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:207)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2695)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2902)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:824)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:783)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:792)",
      "  ... 37 elided",
      "Caused by: java.lang.NullPointerException",
      "  at lowerRemoveAllSpaces(<console>:25)",
      "  at $anonfun$lowerRemoveAllSpacesUDF$1(<console>:28)",
      "  at org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$f$2(ScalaUDF.scala:156)",
      "  at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1126)",
      "  ... 126 more",
      ""
     ]
    }
   ],
   "source": [
    "def lowerRemoveAllSpaces(s:String) : String = {\n",
    "    s.toLowerCase().replaceAll(\"\\\\s\",\"\")\n",
    "}\n",
    "\n",
    "val lowerRemoveAllSpacesUDF = udf[String,String](lowerRemoveAllSpaces)\n",
    "\n",
    "val anotherDF = Seq((\"    thIS Is tOO. \"),(\" hElLO\"),(null)).toDF(\"word\")\n",
    "anotherDF.select(lowerRemoveAllSpacesUDF($\"word\").as(\"cleanWord\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will fail because of Null Pointer Exception. So we have to write more robust code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|cleanWord|\n",
      "+---------+\n",
      "|thisistoo|\n",
      "|    hello|\n",
      "|     null|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "betterLowerRemoveAllSpaces: (s: String)Option[String]\n",
       "betterLowerRemoveAllSpacesUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$2623/1139397239@6e16b21,StringType,List(Some(class[value[0]: string])),None,true,true)\n",
       "anotherDF: org.apache.spark.sql.DataFrame = [word: string]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def betterLowerRemoveAllSpaces(s:String) : Option[String] = {\n",
    "    val str = Option(s).getOrElse(return None)\n",
    "    Some(str.toLowerCase().replaceAll(\"\\\\s\",\"\"))\n",
    "}\n",
    "\n",
    "val betterLowerRemoveAllSpacesUDF = udf[Option[String],String](betterLowerRemoveAllSpaces)\n",
    "\n",
    "val anotherDF = Seq((\"    thIS Is tOO \"),(\" hElLO\"),(null)).toDF(\"word\")\n",
    "anotherDF.select(betterLowerRemoveAllSpacesUDF($\"word\").as(\"cleanWord\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better code. But now we will write the same function in native spark library and help write a better optimized code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|cleanWord|\n",
      "+---------+\n",
      "|thisistoo|\n",
      "|    hello|\n",
      "|     null|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Column\n",
       "bestLowerRemoveAllSpaces: ()(col: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "\n",
    "def bestLowerRemoveAllSpaces()(col : Column ): Column = {\n",
    "    lower(regexp_replace(col,\"\\\\s\",\"\"))\n",
    "}\n",
    "\n",
    "anotherDF.select(bestLowerRemoveAllSpaces()($\"word\").as(\"cleanWord\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
